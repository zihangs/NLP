{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Homework 4: Language Modelling in Hangman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Student Name: Zihang Su\n",
    "\n",
    "Student ID: 710118\n",
    "\n",
    "Python version used: Python 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## General info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>Due date</b>: 11pm, Wednesday May 2nd\n",
    "\n",
    "<b>Submission method</b>: see LMS\n",
    "\n",
    "<b>Submission materials</b>: completed copy of this iPython notebook\n",
    "\n",
    "<b>Late submissions</b>: -20% per day\n",
    "\n",
    "<b>Marks</b>: 5% of mark for class\n",
    "\n",
    "<b>Overview</b>: In this homework, you'll be creating an 'artificial intelligence' player for the classic Hangman word guessing game. You will need to implement several different automatic strategies based on character level language models, ranging from unigram approaches to higher over n-gram models. Your objective is to create an automatic player which makes the fewest mistakes.\n",
    "\n",
    "<b>Materials</b>: See the main class LMS page for information on the basic setup required for this class, including an iPython notebook viewer and the python packages NLTK, Numpy, Scipy, Matplotlib, Scikit-Learn, and Gensim. In particular, if you are not using a lab computer which already has it installed, we recommend installing all the data for NLTK, since you will need various parts of it to complete this assignment. You can also use any Python built-in packages, but do not use any other 3rd party packages; if your iPython notebook doesn't run on the marker's machine, you will lose marks.  \n",
    "\n",
    "<b>Evaluation</b>: Your iPython notebook should run end-to-end without any errors in a reasonable amount of time, and you must follow all instructions provided below, including specific implementation requirements and instructions for what needs to be printed (please avoid printing output we don't ask for). You should leave the output from running your code in the iPython notebook you submit, to assist with marking. The amount each section is worth is given in parenthesis after the instructions. You will be marked not only on the correctness of your methods, but also the quality and efficency of your code: in particular, you should be careful to use Python built-in functions and operators when appropriate and pick descriptive variable names that adhere to <a href=\"https://www.python.org/dev/peps/pep-0008/\">Python style requirements</a>. If you think it might be unclear what you are doing, you should comment your code to help the marker make sense of it.\n",
    "\n",
    "<b>Extra credit</b>: Each homework has a task which is optional with respect to getting full marks on the assignment, but that can be used to offset any points lost on this or any other homework assignment (but not the final project or the exam). We recommend you skip over this step on your first pass, and come back if you have time: the amount of effort required to receive full marks (1 point) on an extra credit question will be substantially more than earning the same amount of credit on other parts of the homework.\n",
    "\n",
    "<b>Updates</b>: Any major changes to the assignment will be announced via LMS. Minor changes and clarifications will be announced in the forum on LMS, we recommend you check the forum regularly.\n",
    "\n",
    "<b>Academic Misconduct</b>: For most people, collaboration will form a natural part of the undertaking of this homework, and we encourge you to discuss it in general terms with other students. However, this ultimately is still an individual task, and so reuse of code or other instances of clear influence will be considered cheating. We will be checking submissions for originality and will invoke the University’s <a href=\"http://academichonesty.unimelb.edu.au/policy.html\">Academic Misconduct policy</a> where inappropriate levels of collusion or plagiarism are deemed to have taken place.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## The Hangman Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The <a href=\"https://en.wikipedia.org/wiki/Hangman_(game)\">Hangman game</a> is a simple game whereby one person thinks of a word, which they keep secret from their opponent, who tries to guess the word one character at a time. The game ends when the opponent makes more than a fixed number of incorrect guesses, or they figure out the secret word before then (in which case they *win*). \n",
    "\n",
    "Here's a simple version of the game, and a method allowing interactive play. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# allowing better python 2 & python 3 compatibility \n",
    "from __future__ import print_function \n",
    "\n",
    "def hangman(secret_word, guesser, max_mistakes=8, verbose=True, **guesser_args):\n",
    "    \"\"\"\n",
    "        secret_word: a string of lower-case alphabetic characters, i.e., the answer to the game\n",
    "        guesser: a function which guesses the next character at each stage in the game\n",
    "            The function takes a:\n",
    "                mask: what is known of the word, as a string with _ denoting an unknown character\n",
    "                guessed: the set of characters which already been guessed in the game\n",
    "                guesser_args: additional (optional) keyword arguments, i.e., name=value\n",
    "        max_mistakes: limit on length of game, in terms of allowed mistakes\n",
    "        verbose: be chatty vs silent\n",
    "        guesser_args: keyword arguments to pass directly to the guesser function\n",
    "    \"\"\"\n",
    "    secret_word = secret_word.lower()\n",
    "    mask = ['_'] * len(secret_word)\n",
    "    guessed = set()\n",
    "    if verbose:\n",
    "        print(\"Starting hangman game. Target is\", ' '.join(mask), 'length', len(secret_word))\n",
    "    \n",
    "    mistakes = 0\n",
    "    while mistakes < max_mistakes:\n",
    "        if verbose:\n",
    "            print(\"You have\", (max_mistakes-mistakes), \"attempts remaining.\")\n",
    "        guess = guesser(mask, guessed, **guesser_args)\n",
    "\n",
    "        if verbose:\n",
    "            print('Guess is', guess)\n",
    "        if guess in guessed:\n",
    "            if verbose:\n",
    "                print('Already guessed this before.')\n",
    "            mistakes += 1\n",
    "        else:\n",
    "            guessed.add(guess)\n",
    "            \n",
    "            if guess in secret_word:\n",
    "                for i, c in enumerate(secret_word):\n",
    "                    if c == guess:\n",
    "                        mask[i] = c\n",
    "                if verbose:\n",
    "                    print('Good guess:', ' '.join(mask))\n",
    "            \n",
    "            else:\n",
    "                if verbose:\n",
    "                    print('Sorry, try again.')\n",
    "                mistakes += 1\n",
    "                \n",
    "        if '_' not in mask:\n",
    "            if verbose:\n",
    "                print('Congratulations, you won.')\n",
    "            return mistakes\n",
    "        \n",
    "    if verbose:\n",
    "        print('Out of guesses. The word was', secret_word)    \n",
    "    return mistakes\n",
    "\n",
    "def human(mask, guessed, **kwargs):\n",
    "    \"\"\"\n",
    "    simple function for manual play\n",
    "    \"\"\"\n",
    "    print('Enter your guess:')\n",
    "    try:\n",
    "        return raw_input().lower().strip() # python 3\n",
    "    except NameError:\n",
    "        return input().lower().strip() # python 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You can play the game interactively using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your guess:\n",
      "i\n",
      "Enter your guess:\n",
      "n\n",
      "Enter your guess:\n",
      "t\n",
      "Enter your guess:\n",
      "e\n",
      "Enter your guess:\n",
      "g\n",
      "Enter your guess:\n",
      "r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hangman('integer', human, 26, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>Instructions</b>: We will be using the words occurring in the *Brown* corpus for *training* an artificial intelligence guessing algorithm, and for *evaluating* the quality of the method. Note that we are intentionally making the hangman game hard, as the AI will need to cope with test words that it has not seen before, hence it will need to learn generalisable patterns of characters to make reasonable predictions.\n",
    "\n",
    "Your first task is to compute the unique word types occurring in the *Brown* corpus, using `nltk.corpus.Brown`, selecting only words that are entirely comprised of alphabetic characters, and lowercasing the words. Finally, randomly shuffle (`numpy.random.shuffle`) this collection of word types, and split them into disjoint training and testing sets. The test set should contain 1000 word types, and the rest should be in the training set. Your code should print the sizes of the training and test sets.\n",
    "\n",
    "Feel free to test your own Hangman performance using `hangman(numpy.random.choice(test_set), human, 8, True)`. It is surprisingly difficult (and addictive)!\n",
    "\n",
    "(0.5 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set:  39234\n",
      "test set:      1000\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "word_types = set()\n",
    "for sent in brown.sents():\n",
    "    for word in sent:\n",
    "        if re.match(r'^[a-zA-Z]+$', word):\n",
    "            word_types.add(word.lower())\n",
    "\n",
    "number_of_testing_word = 1000\n",
    "word_types = list(word_types)\n",
    "np.random.shuffle(word_types)\n",
    "training_set = word_types[0:-number_of_testing_word]\n",
    "test_set = word_types[-number_of_testing_word:]\n",
    "\n",
    "print('training set:  ' + str(len(training_set)))\n",
    "print('test set:      ' + str(len(test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>Instructions</b>: To set a baseline, your first *AI* attempt will be a trivial random method. For this you should implement a guessing method, similar to the `human` method above, i.e., using the same input arguments and returning a character. Your method should randomly choose a character from the range `'a'...'z'` after excluding the characters that have already been guessed in the current game (all subsequent AI approaches should also exclude previous guesses). You might want to use `numpy.random.choice` for this purpose.\n",
    "\n",
    "To measure the performance of this (and later) techiques, implement a method that measures the average number of mistakes made by this technique over all the words in the `test_set`. You will want to turn off the printouts for this, using the `verbose=False` option, and increase the cap on the game length to `max_mistakes=26`. Print the average number of mistakes for the random AI, which will become a baseline for the following steps.\n",
    "\n",
    "(1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average mistakes for random AI approach:   16.641\n"
     ]
    }
   ],
   "source": [
    "alphabetic = list('abcdefghijklmnopqrstuvwxyz')\n",
    "\n",
    "def first_AI(mask, guessed, **guesser_args):\n",
    "    my_guess = np.random.choice(alphabetic)\n",
    "    while (my_guess in guessed):\n",
    "        my_guess = np.random.choice(alphabetic)\n",
    "    return my_guess\n",
    "\n",
    "def average_length(iteration, guesser):\n",
    "    count = 0\n",
    "    for i in range(iteration):\n",
    "        mistake = hangman(np.random.choice(test_set), guesser, 26, False)\n",
    "        count += mistake\n",
    "    return count/iteration\n",
    "\n",
    "number_of_iteration = 1000\n",
    "average = average_length(number_of_iteration, first_AI)\n",
    "print('Average mistakes for random AI approach:   ' + str(average))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Instructions:** As your first real AI, you should train a *unigram* model over the training set.  This requires you to find the frequencies of characters over all training words. Using this model, you should write a guess function that returns the character with the highest probability, after aggregating (summing) the probability of each blank character in the secret word. Print the average number of mistakes the unigram method makes over the test set. Remember to exclude already guessed characters, and use the same evaluation method as above, so the results are comparable. (Hint: it should be much lower than for random).\n",
    "\n",
    "(1 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average mistakes for unigram:    10.325\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from numpy.random import choice\n",
    "\n",
    "def unigram_count():  # can input as a corpus\n",
    "    character_counts = Counter()\n",
    "    \n",
    "    for word in training_set:\n",
    "        for character in word:\n",
    "            character_counts[character] += 1\n",
    "    return character_counts\n",
    "\n",
    "# We must have unigram_counts before we run this function.\n",
    "def unigram(mask, guessed, **guesser_args):\n",
    "    max_value = 0\n",
    "    for c in alphabetic:\n",
    "        if (c not in guessed) and (unigram_counts[c] >= max_value):\n",
    "            max_value = unigram_counts[c]\n",
    "            character = c\n",
    "    return character\n",
    "\n",
    "\n",
    "unigram_counts = unigram_count()\n",
    "average_with_length = average_length(number_of_iteration, unigram)\n",
    "print('Average mistakes for unigram:    ' + str(average_with_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Instructions:** The length of the secret word is an important clue that we might exploit. Different length words tend to have different distributions over characters, e.g., short words are less likely to have suffixes or prefixes. Your job now is to incorporate this idea by conditioning the unigram model on the length of the secret word, i.e., having *different* unigram models for each length of word. You will need to be a little careful at test time, to be robust to the (unlikely) situation that you encounter a word length that you didn't see in training. Create another AI guessing function using this new model, and print its test performance.   \n",
    "\n",
    "(0.5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without length:  10.604\n",
      "with length:     10.516\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def unigram_count_with_length():  # can input as a corpus\n",
    "    character_counts = defaultdict(Counter)\n",
    "    \n",
    "    for word in training_set:\n",
    "        length = len(word)\n",
    "        for character in word:\n",
    "            character_counts[length][character] += 1\n",
    "            \n",
    "    return character_counts\n",
    "\n",
    "# We must have length_uni_count before we run this function.\n",
    "def length_unigram(mask, guessed, **guesser_args):\n",
    "    length = len(mask)\n",
    "    character = ''\n",
    "    if length in length_uni_count.keys():\n",
    "        max_value = 0\n",
    "        for c in alphabetic:\n",
    "            if (c not in guessed) and (length_uni_count[length][c] >= max_value):\n",
    "                max_value = length_uni_count[length][c]\n",
    "                character = c\n",
    "    else:\n",
    "        max_value = 0\n",
    "        for c in alphabetic:\n",
    "            if (c not in guessed) and (unigram_counts[c] >= max_value):\n",
    "                max_value = unigram_counts[c]\n",
    "                character = c\n",
    "    return character\n",
    "\n",
    "\n",
    "def compare_average_length(iteration, guesser1, guesser2):\n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    for i in range(iteration):\n",
    "        word = np.random.choice(test_set)\n",
    "        mistake1 = hangman(word, guesser1, 26, False)\n",
    "        mistake2 = hangman(word, guesser2, 26, False)\n",
    "        count1 += mistake1\n",
    "        count2 += mistake2\n",
    "    return count1/iteration, count2/iteration\n",
    "\n",
    "unigram_counts = unigram_count()\n",
    "length_uni_count = unigram_count_with_length()\n",
    "average1, average2 = compare_average_length(number_of_iteration, unigram, length_unigram)\n",
    "print('without length:  ' + str(average1))\n",
    "print('with length:     ' + str(average2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Instructions:** Now for the main challenge, using a *ngram* language model over characters. The order of characters is obviously important, yet this wasn't incorporated in any of the above models. Knowing that the word has the sequence `n _ s s` is a pretty strong clue that the missing character might be `e`. Similarly the distribution over characters that start or end a word are highly biased (e.g., toward common prefixes and suffixes, like *un-*, *-ed* and *-ly*).\n",
    "\n",
    "Your job is to develop a *ngram* language model over characters, train this over the training words (being careful to handle the start of each word properly, e.g., by padding with sentinel symbols.) You should use linear interpolation to smooth between the higher order and lower order models, and you will have to decide how to weight each component. \n",
    "\n",
    "Your guessing AI algorithm should apply your language model to each blank position in the secret word by using as much of the left context as is known. E.g., in `_ e c _ e _ _` we know the full left context for the first blank (context=start of word), we have a context of two characters for the second blank (context=ec), one character for the second last blank (context=e), and no known context for the last one. If we were using a *n=3* order model, we would be able to apply it to the first and second blanks, but would only be able to use the bigram or unigram distributions for the subsequent blanks. As with the unigram model, you should sum over the probability distributions for each blank to find the expected count for each character type, then select the  character with the highest expected count.\n",
    "\n",
    "Implement the ngram method for *n=3,4* and *5* and evaluate each of these three models over the test set. Do you see any improvement over the unigram methods above?\n",
    "\n",
    "(2 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average mistakes(3-gram):  8.175\n",
      "Average mistakes(4-gram):  7.869\n",
      "Average mistakes(5-gram):  7.745\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def unigram_count_with_start_end():  # can input as a corpus\n",
    "    character_counts = Counter()\n",
    "    \n",
    "    for word in training_set:\n",
    "        word = [c.lower() for c in word]\n",
    "        for character in word:\n",
    "            character_counts[character] += 1\n",
    "    return character_counts\n",
    "\n",
    "\n",
    "def bigram_count():\n",
    "    character_counts = defaultdict(Counter)\n",
    "    for word in training_set:\n",
    "        word = [\"<s1>\"] + [c.lower() for c in word] + [\"</s1>\"]\n",
    "        bigram_list = zip(word[:-1], word[1:])\n",
    "        for bigram in bigram_list:\n",
    "            first, second = bigram\n",
    "            character_counts[first][second] += 1\n",
    "    \n",
    "    return character_counts\n",
    "\n",
    "def trigram_count():\n",
    "    character_counts = defaultdict(lambda: defaultdict(Counter))\n",
    "    for word in training_set:\n",
    "        word = [\"<s1>\", \"<s2>\"] + [c.lower() for c in word] + [\"</s1>\", \"</s2>\"]\n",
    "        trigram_list = zip(word[:-2], word[1:-1], word[2:])\n",
    "        for trigram in trigram_list:\n",
    "            first, second, third = trigram\n",
    "            character_counts[first][second][third] += 1\n",
    "    \n",
    "    return character_counts\n",
    "\n",
    "def four_gram_count():\n",
    "    character_counts = defaultdict(lambda: defaultdict(lambda: defaultdict(Counter)))\n",
    "    for word in training_set:\n",
    "        word = [\"<s1>\", \"<s2>\", \"<s3>\"] + [c.lower() for c in word] + [\"</s1>\", \"</s2>\", \"</s3>\"]\n",
    "        four_gram_list = zip(word[:-3], word[1:-2], word[2:-1], word[3:])\n",
    "        for four_gram in four_gram_list:\n",
    "            first, second, third, fourth = four_gram\n",
    "            character_counts[first][second][third][fourth] += 1\n",
    "    \n",
    "    return character_counts\n",
    "\n",
    "def five_gram_count():\n",
    "    character_counts = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(Counter))))\n",
    "    for word in training_set:\n",
    "        word = [\"<s1>\", \"<s2>\", \"<s3>\", \"<s4>\"] + [c.lower() for c in word] + [\"</s1>\", \"</s2>\", \"</s3>\", \"</s4>\"]\n",
    "        five_gram_list = zip(word[:-4], word[1:-3], word[2:-2], word[3:-1], word[4:])\n",
    "        for five_gram in five_gram_list:\n",
    "            first, second, third, fourth, fifth = five_gram\n",
    "            character_counts[first][second][third][fourth][fifth] += 1\n",
    "    \n",
    "    return character_counts\n",
    "\n",
    "ngram_count_1 = unigram_count_with_start_end()\n",
    "ngram_count_2 = bigram_count()\n",
    "ngram_count_3 = trigram_count()\n",
    "ngram_count_4 = four_gram_count()\n",
    "ngram_count_5 = five_gram_count()\n",
    "\n",
    "def unigram_prob(word):\n",
    "    denominator = sum(ngram_count_1.values())\n",
    "    if denominator != 0:\n",
    "        prob = ngram_count_1[word]/denominator\n",
    "    else:\n",
    "        prob = 0.0\n",
    "    return prob\n",
    "\n",
    "def bigram_prob(prev1, word):   # input are two words\n",
    "    denominator = sum(ngram_count_2[prev1].values())\n",
    "    if denominator != 0:\n",
    "        prob = 0.5*(ngram_count_2[prev1][word]/denominator)\n",
    "        + 0.5*(unigram_prob(word))\n",
    "    else:\n",
    "        prob = 0.5*(unigram_prob(word))\n",
    "    return prob\n",
    "\n",
    "def trigram_prob(prev2, prev1, word):\n",
    "    denominator = sum(ngram_count_3[prev2][prev1].values())\n",
    "    if denominator != 0:\n",
    "        prob = 0.33*(ngram_count_3[prev2][prev1][word]/denominator)\n",
    "        + 0.67*(bigram_prob(prev1, word))\n",
    "    else:\n",
    "        prob = 0.67*(bigram_prob(prev1, word))\n",
    "    return prob\n",
    "\n",
    "def four_gram_prob(prev3, prev2, prev1, word):\n",
    "    denominator = sum(ngram_count_4[prev3][prev2][prev1].values())\n",
    "    if denominator != 0:\n",
    "        prob = 0.25*(ngram_count_4[prev3][prev2][prev1][word]/denominator)\n",
    "        + 0.75*(trigram_prob(prev2, prev1, word))\n",
    "    else:\n",
    "        prob = 0.75*(trigram_prob(prev2, prev1, word))\n",
    "    return prob\n",
    "\n",
    "def five_gram_prob(prev4, prev3, prev2, prev1, word):\n",
    "    denominator = sum(ngram_count_5[prev4][prev3][prev2][prev1].values())\n",
    "    if denominator != 0:\n",
    "        prob = 0.2*(ngram_count_5[prev4][prev3][prev2][prev1][word]/denominator)\n",
    "        + 0.8*(four_gram_prob(prev3, prev2, prev1, word))\n",
    "    else:\n",
    "        prob = 0.8*(four_gram_prob(prev3, prev2, prev1, word))\n",
    "    return prob\n",
    "\n",
    "def bigram(mask, guessed, **guesser_args):\n",
    "    mask = [\"<s1>\"] + mask + [\"</s1>\"]\n",
    "    length = len(mask)\n",
    "    prob_matrix = np.zeros((length, 26))\n",
    "\n",
    "    for i in range(1,length-1):\n",
    "        if mask[i] == '_':\n",
    "            prev_1 = mask[i-1]\n",
    "            for j in range(26):\n",
    "                current = alphabetic[j]\n",
    "                prob_matrix[i][j] = bigram_prob(prev_1, current)\n",
    "\n",
    "    max_value = 0\n",
    "    for m in range(26):\n",
    "        score = sum(prob_matrix[:, m])\n",
    "        c = alphabetic[m]\n",
    "        if (c not in guessed) and (score >= max_value):\n",
    "            max_value = score\n",
    "            character = c\n",
    "    return character\n",
    "\n",
    "\n",
    "def trigram(mask, guessed, **guesser_args):\n",
    "    mask = [\"<s1>\", \"<s2>\"] + mask + [\"</s1>\", \"</s2>\"]\n",
    "    length = len(mask)\n",
    "    prob_matrix = np.zeros((length, 26))\n",
    "\n",
    "    for i in range(2,length-2):\n",
    "        if mask[i] == '_':\n",
    "            prev_1 = mask[i-1]\n",
    "            prev_2 = mask[i-2]\n",
    "            for j in range(26):\n",
    "                current = alphabetic[j]\n",
    "                prob_matrix[i][j] = trigram_prob(prev_2, prev_1, current)\n",
    "                \n",
    "    max_value = 0\n",
    "    for m in range(26):\n",
    "        score = sum(prob_matrix[:, m])\n",
    "        c = alphabetic[m]\n",
    "        if (c not in guessed) and (score >= max_value):\n",
    "            max_value = score\n",
    "            character = c\n",
    "    return character\n",
    "\n",
    "\n",
    "def four_gram(mask, guessed, **guesser_args):\n",
    "    mask = [\"<s1>\", \"<s2>\", \"<s3>\"] + mask + [\"</s1>\", \"</s2>\", \"</s3>\"]\n",
    "    length = len(mask)\n",
    "    prob_matrix = np.zeros((length, 26))\n",
    "\n",
    "    for i in range(3,length-3):\n",
    "        if mask[i] == '_':\n",
    "            prev_1 = mask[i-1]\n",
    "            prev_2 = mask[i-2]\n",
    "            prev_3 = mask[i-3]\n",
    "            for j in range(26):\n",
    "                current = alphabetic[j]\n",
    "                prob_matrix[i][j] = four_gram_prob(prev_3, prev_2, prev_1, current)\n",
    "                \n",
    "    max_value = 0\n",
    "    for m in range(26):\n",
    "        score = sum(prob_matrix[:, m])\n",
    "        c = alphabetic[m]\n",
    "        if (c not in guessed) and (score >= max_value):\n",
    "            max_value = score\n",
    "            character = c\n",
    "    return character\n",
    "\n",
    "def five_gram(mask, guessed, **guesser_args):\n",
    "    mask = [\"<s1>\", \"<s2>\", \"<s3>\", \"<s4>\"] + mask + [\"</s1>\", \"</s2>\", \"</s3>\", \"</s4>\"]\n",
    "    length = len(mask)\n",
    "    prob_matrix = np.zeros((length, 26))\n",
    "\n",
    "    for i in range(4,length-4):\n",
    "        if mask[i] == '_':\n",
    "            prev_1 = mask[i-1]\n",
    "            prev_2 = mask[i-2]\n",
    "            prev_3 = mask[i-3]\n",
    "            prev_4 = mask[i-4]\n",
    "            for j in range(26):\n",
    "                current = alphabetic[j]\n",
    "                prob_matrix[i][j] = five_gram_prob(prev_4, prev_3, prev_2, prev_1, current)\n",
    "                \n",
    "    max_value = 0\n",
    "    for m in range(26):\n",
    "        score = sum(prob_matrix[:, m])\n",
    "        c = alphabetic[m]\n",
    "        if (c not in guessed) and (score >= max_value):\n",
    "            max_value = score\n",
    "            character = c\n",
    "    return character\n",
    "\n",
    "\n",
    "ngram_3 = average_length(number_of_iteration, trigram)\n",
    "ngram_4 = average_length(number_of_iteration, four_gram)\n",
    "ngram_5 = average_length(number_of_iteration, five_gram)\n",
    "\n",
    "print('Average mistakes(3-gram):  ' + str(ngram_3))\n",
    "print('Average mistakes(4-gram):  ' + str(ngram_4))\n",
    "print('Average mistakes(5-gram):  ' + str(ngram_5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Bonus: Improving the AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Instructions:** To get the extra credit, you should try to develop a more effective AI for hangman. Feel free to engage your creativity here! Possibilities include better conditioning on the length of the word and the parts that are known, fancier smoothing methods, using backwards ngram models, or a fancier inference algorithm like forward-back using a HMM. Ensure you report the test performance of your method.\n",
    "\n",
    "You will be marked based on the ambition of your approach and on its accuracy. If you have tried some truly spectacular method but it didn't really work, then please include your implementation and an explanation, which will still attract marks for ambition.\n",
    "\n",
    "(1 bonus mark) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
